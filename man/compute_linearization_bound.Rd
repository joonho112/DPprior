% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/13_error_bounds.R
\name{compute_linearization_bound}
\alias{compute_linearization_bound}
\title{Mean-Linearization Error Bound}
\usage{
compute_linearization_bound(J, alpha, cJ = log(J))
}
\arguments{
\item{J}{Integer; sample size.}

\item{alpha}{Numeric; concentration parameter (vectorized).}

\item{cJ}{Numeric; scaling constant (default: log(J)).}
}
\value{
Numeric; upper bound via Pinsker's inequality.
}
\description{
Computes an upper bound on the TV distance between two Poisson distributions,
\eqn{\text{Poisson}(\lambda_J(\alpha))} and \eqn{\text{Poisson}(\alpha c_J)},
using the Poisson-Poisson KL divergence together with Pinsker's inequality.
}
\details{
Let \eqn{\lambda = \lambda_J(\alpha)} (exact shifted mean) and
\eqn{\lambda' = \alpha c_J} (A1 approximate mean).

The KL divergence is:
\deqn{KL(\text{Poisson}(\lambda) || \text{Poisson}(\lambda')) =
      \lambda \log(\lambda/\lambda') + \lambda' - \lambda}

By Pinsker's inequality:
\deqn{d_{TV}(\text{Poisson}(\lambda), \text{Poisson}(\lambda')) \le \sqrt{KL/2}}

Numerical safeguards handle edge cases where \eqn{\lambda} or \eqn{c_J} is zero.
}
\examples{
\dontrun{
# Linearization bound for J=50, alpha=1
compute_linearization_bound(J = 50, alpha = 1)

# Effect of J on linearization bound (should decrease)
sapply(c(25, 50, 100, 200), function(J)
  compute_linearization_bound(J, alpha = 2))

}
}
\references{
Lee, J. (2026). Design-Conditional Prior Elicitation for Dirichlet Process Mixtures.
\emph{arXiv preprint} arXiv:2602.06301.
}
\seealso{
\code{\link{compute_poissonization_bound}}, \code{\link{compute_total_tv_bound}}
}
\keyword{internal}
